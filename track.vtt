WEBVTT

00:00:00.359 --> 00:00:03.179
should we automate away all the jobs

00:00:03.179 --> 00:00:04.980
including the fulfilling ones should we

00:00:04.980 --> 00:00:06.899
allow AI machines to flood the internet

00:00:06.899 --> 00:00:09.240
with propaganda and fake news should we

00:00:09.240 --> 00:00:12.120
develop non-human Minds smarter than our

00:00:12.120 --> 00:00:13.920
own machines that might one day

00:00:13.920 --> 00:00:17.279
outnumber us or outsmart us do we risk

00:00:17.279 --> 00:00:20.279
losing control now you might think that

00:00:20.279 --> 00:00:22.320
sounds like some futuristic script from

00:00:22.320 --> 00:00:24.600
a Terminator movie but last month some

00:00:24.600 --> 00:00:26.279
of the most well-known figures who are

00:00:26.279 --> 00:00:28.199
involved in the development and training

00:00:28.199 --> 00:00:30.420
of artificial intelligence call for a

00:00:30.420 --> 00:00:32.159
moratorium until we better understand

00:00:32.159 --> 00:00:35.040
where we're going an open letter was

00:00:35.040 --> 00:00:36.719
signed by thousands of entrepreneurs

00:00:36.719 --> 00:00:39.059
academics and scientists including Elon

00:00:39.059 --> 00:00:40.980
Musk who wants the training of

00:00:40.980 --> 00:00:43.020
intelligence haltered for at least six

00:00:43.020 --> 00:00:45.300
months we're going to dig deep into this

00:00:45.300 --> 00:00:47.700
over the next 20 minutes or so in the

00:00:47.700 --> 00:00:49.620
company of two people who know a thing

00:00:49.620 --> 00:00:52.320
or two about it joining me is the tech

00:00:52.320 --> 00:00:55.079
investor even burfield the Evan burfield

00:00:55.079 --> 00:00:58.140
the author of regulatory hacking A

00:00:58.140 --> 00:01:00.719
playbook for startups he's in Texas I'm

00:01:00.719 --> 00:01:03.180
Professor Gary Marcus is in Vancouver

00:01:03.180 --> 00:01:05.580
he's the professor emeritus at New York

00:01:05.580 --> 00:01:08.760
University and author of rebooting AI

00:01:08.760 --> 00:01:11.400
Professor let me start with you

00:01:11.400 --> 00:01:14.280
um clearly with such advances that we're

00:01:14.280 --> 00:01:16.920
seeing we have to set some guard rails

00:01:16.920 --> 00:01:18.659
who do you think should be in charge of

00:01:18.659 --> 00:01:20.280
that

00:01:20.280 --> 00:01:22.560
uh were we both professors I think that

00:01:22.560 --> 00:01:25.439
we need Global governance for AI I think

00:01:25.439 --> 00:01:28.020
that we have a lot of patchworks right

00:01:28.020 --> 00:01:29.820
now almost balkanized

00:01:29.820 --> 00:01:31.200
um the worst case from the company's

00:01:31.200 --> 00:01:32.640
perspective and the world's perspective

00:01:32.640 --> 00:01:35.759
is if there's 193 jurisdictions each

00:01:35.759 --> 00:01:37.680
deciding their own rules requiring their

00:01:37.680 --> 00:01:39.780
own training of these models

00:01:39.780 --> 00:01:41.700
um each run by governments that don't

00:01:41.700 --> 00:01:44.340
have much specific expertise in AI so

00:01:44.340 --> 00:01:46.920
what I called for an economist editorial

00:01:46.920 --> 00:01:49.200
earlier this weekend in a TED talk

00:01:49.200 --> 00:01:51.000
earlier this week was to have a global

00:01:51.000 --> 00:01:52.860
system modeled on something like the

00:01:52.860 --> 00:01:54.840
international atomic energy Authority

00:01:54.840 --> 00:01:56.520
where the world comes together and says

00:01:56.520 --> 00:01:58.680
we have a new threat here but it's

00:01:58.680 --> 00:02:00.180
really a new set of threats and we need

00:02:00.180 --> 00:02:02.040
to work together on this so I think the

00:02:02.040 --> 00:02:03.600
number one thing is it should be Global

00:02:03.600 --> 00:02:05.340
and the number two thing is it can't be

00:02:05.340 --> 00:02:06.360
just

00:02:06.360 --> 00:02:07.979
um policy but it also has to be a

00:02:07.979 --> 00:02:10.139
research side because we need to invent

00:02:10.139 --> 00:02:12.120
new tools like we had to invent for

00:02:12.120 --> 00:02:14.400
fighting spam and cyber warfare and so

00:02:14.400 --> 00:02:16.140
forth there's so many different threats

00:02:16.140 --> 00:02:17.340
as you mentioned around misinformation

00:02:17.340 --> 00:02:19.980
cyber crime and so forth so we need to

00:02:19.980 --> 00:02:22.200
have a kind of standing organization

00:02:22.200 --> 00:02:24.780
This Global and well-financed to try to

00:02:24.780 --> 00:02:26.940
build tools to mitigate those threats so

00:02:26.940 --> 00:02:29.580
Evan burfield there are many many people

00:02:29.580 --> 00:02:31.140
who who just want to press the pause

00:02:31.140 --> 00:02:33.239
button until we work out some of these

00:02:33.239 --> 00:02:35.340
things but I can already see and I've

00:02:35.340 --> 00:02:38.700
heard uh the reasons why that probably

00:02:38.700 --> 00:02:40.379
isn't possible and and that is because

00:02:40.379 --> 00:02:42.900
not everybody will stop and people are

00:02:42.900 --> 00:02:44.700
worried about losing competitive

00:02:44.700 --> 00:02:47.519
Advantage so how do we best go about

00:02:47.519 --> 00:02:49.319
this

00:02:49.319 --> 00:02:51.780
yeah I think that's exactly right

00:02:51.780 --> 00:02:53.760
um you know the the challenge with a

00:02:53.760 --> 00:02:55.500
moratorium is that it's incredibly hard

00:02:55.500 --> 00:02:56.819
to enforce

00:02:56.819 --> 00:02:58.739
um the responsible actors would be more

00:02:58.739 --> 00:03:00.720
likely to follow it the irresponsible

00:03:00.720 --> 00:03:02.879
actors wouldn't but that's actually not

00:03:02.879 --> 00:03:04.580
so much my concern with the moratorium

00:03:04.580 --> 00:03:07.560
there's absolutely questions we need to

00:03:07.560 --> 00:03:09.180
be asking about the governance of AI

00:03:09.180 --> 00:03:11.099
about what industry can do what

00:03:11.099 --> 00:03:13.260
government can do uh I think the letter

00:03:13.260 --> 00:03:15.300
did spark a conversation Schumer is

00:03:15.300 --> 00:03:17.819
working on a new AI bill here in the U.S

00:03:17.819 --> 00:03:19.500
rumors McCarthy's working on a

00:03:19.500 --> 00:03:20.879
republican version

00:03:20.879 --> 00:03:23.879
but what I think is actually much more

00:03:23.879 --> 00:03:26.459
important is to start to have the

00:03:26.459 --> 00:03:28.680
conversations about how we prepare our

00:03:28.680 --> 00:03:31.500
society our economy uh our political

00:03:31.500 --> 00:03:35.099
system democracy itself for all of the

00:03:35.099 --> 00:03:37.500
implications of AI that are coming one

00:03:37.500 --> 00:03:40.620
way or another and I suspect we'll see a

00:03:40.620 --> 00:03:42.540
year from now we'll go this was less

00:03:42.540 --> 00:03:44.280
impactful than we thought five years

00:03:44.280 --> 00:03:47.099
from now it will be an absolute tsunami

00:03:47.099 --> 00:03:49.080
of upheaval and we have this window

00:03:49.080 --> 00:03:50.580
right now where we can have this

00:03:50.580 --> 00:03:52.440
conversation and we can get creative and

00:03:52.440 --> 00:03:55.019
I think we've got to use it and a

00:03:55.019 --> 00:03:56.580
moratorium gives us this false sense of

00:03:56.580 --> 00:03:58.500
security that we have control and can

00:03:58.500 --> 00:04:00.840
stop it versus figuring out how we ride

00:04:00.840 --> 00:04:02.700
this tsunami and try to direct it in a

00:04:02.700 --> 00:04:04.680
much better Direction Professor Marcus

00:04:04.680 --> 00:04:06.120
did we

00:04:06.120 --> 00:04:10.140
did we learn anything from the the last

00:04:10.140 --> 00:04:12.599
technological Advance the advance of the

00:04:12.599 --> 00:04:15.720
internet of social media are there

00:04:15.720 --> 00:04:17.880
lessons from that which we let's face it

00:04:17.880 --> 00:04:19.199
we didn't do very well that are

00:04:19.199 --> 00:04:20.940
applicable here

00:04:20.940 --> 00:04:22.620
I think the number one lesson is you

00:04:22.620 --> 00:04:24.000
don't want to close the Barn Door after

00:04:24.000 --> 00:04:26.280
the horse has left I think you know

00:04:26.280 --> 00:04:29.160
we're very late in in figuring out what

00:04:29.160 --> 00:04:31.320
to do about social media I think we

00:04:31.320 --> 00:04:33.600
probably handled privacy alone in the

00:04:33.600 --> 00:04:34.680
wrong way

00:04:34.680 --> 00:04:36.479
um we wound up with so much polarization

00:04:36.479 --> 00:04:38.639
and hostility we wound up with

00:04:38.639 --> 00:04:40.320
misinformation

00:04:40.320 --> 00:04:42.660
um I think we waited too long to act I

00:04:42.660 --> 00:04:44.820
think the number one lesson is we should

00:04:44.820 --> 00:04:47.220
get on it right now and I agree with the

00:04:47.220 --> 00:04:49.320
other panelists that the moratorium You

00:04:49.320 --> 00:04:50.940
could argue about the merits whether it

00:04:50.940 --> 00:04:52.199
was the right thing or the wrong thing

00:04:52.199 --> 00:04:54.000
it was absolutely the right thing to

00:04:54.000 --> 00:04:55.620
raise this and get it on everybody's

00:04:55.620 --> 00:04:57.720
agenda this is not something we want six

00:04:57.720 --> 00:05:00.479
months from now something we need now so

00:05:00.479 --> 00:05:02.699
Evan burfield when you talk about a

00:05:02.699 --> 00:05:04.500
tsunami in five years time what does

00:05:04.500 --> 00:05:06.900
what does that look like

00:05:06.900 --> 00:05:09.840
uh look I'm down here in Austin Texas uh

00:05:09.840 --> 00:05:11.699
at Capitol Factory startup accelerate

00:05:11.699 --> 00:05:13.979
I've spent the whole day uh you know

00:05:13.979 --> 00:05:15.479
meeting with startups and there's not a

00:05:15.479 --> 00:05:16.860
startup right now out there that is not

00:05:16.860 --> 00:05:19.320
applying these AI generative models

00:05:19.320 --> 00:05:21.479
these large language models to every

00:05:21.479 --> 00:05:23.280
interesting problem of the Sun and

00:05:23.280 --> 00:05:26.400
there's all of the the scary dystopian

00:05:26.400 --> 00:05:29.100
possibilities that you uh led into this

00:05:29.100 --> 00:05:31.259
segment with but there's also uh

00:05:31.259 --> 00:05:33.419
incredible advances in how to make work

00:05:33.419 --> 00:05:35.400
more fulfilling and more impactful how

00:05:35.400 --> 00:05:38.340
to apply uh tremendous personalization

00:05:38.340 --> 00:05:40.320
to Medicine based on our genetics our

00:05:40.320 --> 00:05:42.060
environment the particular issues we're

00:05:42.060 --> 00:05:44.100
having how do you make government more

00:05:44.100 --> 00:05:45.539
responsive and feel more like a

00:05:45.539 --> 00:05:47.400
concierge to Citizens all of that is

00:05:47.400 --> 00:05:49.080
also being worked on

00:05:49.080 --> 00:05:51.419
um and I think figuring out how we put

00:05:51.419 --> 00:05:53.160
the guard rails in place around some of

00:05:53.160 --> 00:05:55.020
the scarier things which isn't just

00:05:55.020 --> 00:05:57.060
about regulating Aya it's about changing

00:05:57.060 --> 00:05:58.979
our social policy changing our Market

00:05:58.979 --> 00:06:00.840
policies themselves

00:06:00.840 --> 00:06:02.639
so that we can mitigate some of that and

00:06:02.639 --> 00:06:05.039
and direct this into the the much more

00:06:05.039 --> 00:06:07.199
hopeful and optimistic Direction why why

00:06:07.199 --> 00:06:10.500
on that point though Evan is it is it

00:06:10.500 --> 00:06:12.600
imaginable in the current scenario you

00:06:12.600 --> 00:06:14.039
are around it all the time that a

00:06:14.039 --> 00:06:16.620
research lab would cross a critical line

00:06:16.620 --> 00:06:18.840
here without even noticing

00:06:18.840 --> 00:06:21.720
I I'm I'm personally skeptic you know uh

00:06:21.720 --> 00:06:24.120
Gary's written some wonderful points

00:06:24.120 --> 00:06:26.039
about the fact that we are we are very

00:06:26.039 --> 00:06:28.319
very far I believe from artificial

00:06:28.319 --> 00:06:30.060
general intelligence and the Terminator

00:06:30.060 --> 00:06:32.160
scenarios I I think we've got to be very

00:06:32.160 --> 00:06:34.560
aware of right now is simply that this

00:06:34.560 --> 00:06:37.319
technology is already right now today at

00:06:37.319 --> 00:06:38.940
a state if it did not Advance any

00:06:38.940 --> 00:06:41.639
further where its application is going

00:06:41.639 --> 00:06:44.340
to profoundly change how we live our

00:06:44.340 --> 00:06:47.400
lives how we work how we engage with

00:06:47.400 --> 00:06:49.020
each other in communities how our

00:06:49.020 --> 00:06:51.419
democracies function uh the impacts on

00:06:51.419 --> 00:06:52.620
our democracies are going to be felt

00:06:52.620 --> 00:06:54.600
right in the 2024 political cycle here

00:06:54.600 --> 00:06:56.400
in the U.S that's what I think we need

00:06:56.400 --> 00:06:58.039
to be talking about and preparing for

00:06:58.039 --> 00:07:01.259
the scenarios of AI is like nuclear

00:07:01.259 --> 00:07:03.419
weapons we have to ban it immediately I

00:07:03.419 --> 00:07:05.160
think are much less applicable to the

00:07:05.160 --> 00:07:08.280
the much more realistic uh changes that

00:07:08.280 --> 00:07:09.840
are already happening around us right

00:07:09.840 --> 00:07:12.539
now that are going to accelerate miles

00:07:12.539 --> 00:07:13.800
um you've just come back from Washington

00:07:13.800 --> 00:07:15.600
and I know that you've been talking to

00:07:15.600 --> 00:07:17.639
policymakers about the specific issues

00:07:17.639 --> 00:07:18.660
in fact the reason we're talking about

00:07:18.660 --> 00:07:21.060
it tonight is because you tweeted no one

00:07:21.060 --> 00:07:23.280
has a clue I I mean is that is it as

00:07:23.280 --> 00:07:24.780
blunt as that that nobody really

00:07:24.780 --> 00:07:26.759
understands it there is practically no

00:07:26.759 --> 00:07:28.440
work being done on it

00:07:28.440 --> 00:07:31.560
hi Christian you're you're spot on the

00:07:31.560 --> 00:07:33.419
three biggest challenges right now with

00:07:33.419 --> 00:07:35.940
policy makers are one this was

00:07:35.940 --> 00:07:37.620
completely foreseeable there were some

00:07:37.620 --> 00:07:39.180
of us in Washington talking about this

00:07:39.180 --> 00:07:41.880
10 or 15 years ago policymakers weren't

00:07:41.880 --> 00:07:43.919
paying attention and most of the think

00:07:43.919 --> 00:07:46.139
tanks in Washington really failed to

00:07:46.139 --> 00:07:47.940
start a conversation about the Practical

00:07:47.940 --> 00:07:49.680
things that needed to be done to prepare

00:07:49.680 --> 00:07:51.960
for the age of AI so we're behind the

00:07:51.960 --> 00:07:54.720
ball from a policy making standpoint the

00:07:54.720 --> 00:07:56.280
second thing I would emphasize what Evan

00:07:56.280 --> 00:07:58.199
burfield just said there is a wave

00:07:58.199 --> 00:07:59.880
coming and you can do two things when a

00:07:59.880 --> 00:08:01.919
wave is coming you can get crushed by it

00:08:01.919 --> 00:08:04.020
or you can ride the wave and to use

00:08:04.020 --> 00:08:06.360
another analogy right now the discussion

00:08:06.360 --> 00:08:08.340
in Washington is about whether to put

00:08:08.340 --> 00:08:10.080
the genie back in the bottle or not that

00:08:10.080 --> 00:08:11.699
shouldn't be the discussion it should be

00:08:11.699 --> 00:08:13.680
what three wishes should we ask the

00:08:13.680 --> 00:08:15.780
genie and that's the discussion that

00:08:15.780 --> 00:08:17.819
should be had about how to handle Ai and

00:08:17.819 --> 00:08:20.460
use it for good purposes and finally the

00:08:20.460 --> 00:08:22.560
other problem is policy makers are not

00:08:22.560 --> 00:08:24.120
thinking two steps forward on the

00:08:24.120 --> 00:08:27.060
chessboard it's AI right now but in in

00:08:27.060 --> 00:08:28.440
this decade AI is going to be

00:08:28.440 --> 00:08:30.720
supercharged by other Technologies like

00:08:30.720 --> 00:08:32.880
Quantum Computing that are going to give

00:08:32.880 --> 00:08:35.640
machines genuine human-like emotion what

00:08:35.640 --> 00:08:37.260
are we doing to prepare for that we

00:08:37.260 --> 00:08:39.000
should be having that conversation now

00:08:39.000 --> 00:08:40.860
there needs to be institutions in

00:08:40.860 --> 00:08:44.099
Washington that focus on that so Jack

00:08:44.099 --> 00:08:46.380
we've had a discussion about that in

00:08:46.380 --> 00:08:48.660
this country and the UK government has

00:08:48.660 --> 00:08:50.660
decided that it doesn't need a dedicated

00:08:50.660 --> 00:08:55.140
UK regulator for AI so who's overseeing

00:08:55.140 --> 00:08:56.519
it

00:08:56.519 --> 00:08:59.399
that's a very good question I mean uh I

00:08:59.399 --> 00:09:01.500
I was on stage last night with the

00:09:01.500 --> 00:09:03.120
chancellor Jeremy Hunt and I asked him

00:09:03.120 --> 00:09:04.500
about this you know he's the guy in

00:09:04.500 --> 00:09:06.839
charge of the UK economy

00:09:06.839 --> 00:09:08.940
um and he was really quite dismissive

00:09:08.940 --> 00:09:11.160
he's he in in the sense that he said you

00:09:11.160 --> 00:09:12.420
know this is something that is going to

00:09:12.420 --> 00:09:14.640
happen and we have always embraced new

00:09:14.640 --> 00:09:16.019
technologies in this country and we

00:09:16.019 --> 00:09:18.600
should do so again uh it's full steam

00:09:18.600 --> 00:09:21.899
ahead was the phrase that he used

00:09:21.899 --> 00:09:24.120
um you know he was very very positive he

00:09:24.120 --> 00:09:25.800
did he did not want to talk about the

00:09:25.800 --> 00:09:27.240
possibility that people would lose their

00:09:27.240 --> 00:09:28.980
jobs because of this technology he only

00:09:28.980 --> 00:09:30.839
saw it as a purely positive thing and he

00:09:30.839 --> 00:09:32.760
was not keen to talk about the way it

00:09:32.760 --> 00:09:34.800
should be regulated now you know I'm no

00:09:34.800 --> 00:09:36.660
expert on this still if I'm a politics

00:09:36.660 --> 00:09:39.000
guy but what I do know is that is how

00:09:39.000 --> 00:09:41.160
Westminster works and how um political

00:09:41.160 --> 00:09:42.959
systems work and I can tell you now and

00:09:42.959 --> 00:09:45.060
you'll know this Christian there is no

00:09:45.060 --> 00:09:47.100
way our political system is set up to

00:09:47.100 --> 00:09:49.440
deal with this challenge absolutely no

00:09:49.440 --> 00:09:51.899
chance the speed at which decisions are

00:09:51.899 --> 00:09:54.600
made in Westminster and I suspect in

00:09:54.600 --> 00:09:57.060
other major political centers is far too

00:09:57.060 --> 00:09:59.459
slow to cope with the pace at which this

00:09:59.459 --> 00:10:01.320
technology is coming the policy makers

00:10:01.320 --> 00:10:03.959
do not understand it at all this is just

00:10:03.959 --> 00:10:05.339
something that is going to wash over us

00:10:05.339 --> 00:10:06.480
and we're going to have to cross our

00:10:06.480 --> 00:10:08.160
fingers you know the UK government put

00:10:08.160 --> 00:10:10.440
out a white paper which is what they

00:10:10.440 --> 00:10:12.540
call their draft strategy on AI the

00:10:12.540 --> 00:10:13.860
other day I mean just the very name of

00:10:13.860 --> 00:10:15.660
it white paper tells you how old school

00:10:15.660 --> 00:10:17.940
this is yeah you know it's out of date

00:10:17.940 --> 00:10:19.620
already and that thing took you know

00:10:19.620 --> 00:10:21.899
years for them to put together

00:10:21.899 --> 00:10:24.420
um we just don't have the sort of nimble

00:10:24.420 --> 00:10:28.680
small system smart thinking people set

00:10:28.680 --> 00:10:30.839
up to deal with this and I'd be very

00:10:30.839 --> 00:10:33.180
surprised if that's different in the US

00:10:33.180 --> 00:10:34.800
or indeed in many of the other big Power

00:10:34.800 --> 00:10:36.300
centers well they clearly don't

00:10:36.300 --> 00:10:38.459
understand it Professor Marcus do they

00:10:38.459 --> 00:10:40.920
call you in to try and get you to

00:10:40.920 --> 00:10:42.540
explain it to them

00:10:42.540 --> 00:10:45.000
that I was talking to people in the U.S

00:10:45.000 --> 00:10:46.560
and Canadian government yesterday I've

00:10:46.560 --> 00:10:48.360
been called a lot lately

00:10:48.360 --> 00:10:50.160
um I think there is an awareness that

00:10:50.160 --> 00:10:51.899
people don't quite know what to do and

00:10:51.899 --> 00:10:54.540
they are increasingly turning to me

00:10:54.540 --> 00:10:56.640
um and also turning to all of my you

00:10:56.640 --> 00:10:58.380
know academic colleagues and so forth so

00:10:58.380 --> 00:11:01.260
I think that there's at least a

00:11:01.260 --> 00:11:03.120
recognition and people know what they

00:11:03.120 --> 00:11:06.000
don't know I do think that the UK white

00:11:06.000 --> 00:11:07.920
paper saying that you won't have a

00:11:07.920 --> 00:11:10.620
central office of AI is certainly for

00:11:10.620 --> 00:11:12.120
all the reasons that were kind of

00:11:12.120 --> 00:11:14.760
implicitly just said which is

00:11:14.760 --> 00:11:16.980
um the government is going to be

00:11:16.980 --> 00:11:18.660
ill-equipped to deal with the speed of

00:11:18.660 --> 00:11:20.220
this and if you just leave it to 20

00:11:20.220 --> 00:11:22.440
different Regulatory Agencies Each of

00:11:22.440 --> 00:11:24.360
which don't have expertise you're asking

00:11:24.360 --> 00:11:26.459
for trouble you're asking for a lack of

00:11:26.459 --> 00:11:28.680
coordination and it's just not realistic

00:11:28.680 --> 00:11:30.000
that all of those agencies are going to

00:11:30.000 --> 00:11:31.920
be up on things so there needs to be I

00:11:31.920 --> 00:11:33.779
think at least some Central oversight I

00:11:33.779 --> 00:11:35.220
think the United States should consider

00:11:35.220 --> 00:11:37.560
a cabinet level AI officer and you

00:11:37.560 --> 00:11:39.600
should consider something comparable

00:11:39.600 --> 00:11:42.540
um you need some people maybe like a G7

00:11:42.540 --> 00:11:44.820
then we have a G7 meeting of foreign

00:11:44.820 --> 00:11:47.120
ministers we need a G7 meeting of of

00:11:47.120 --> 00:11:49.440
IAAI ministers is that is that

00:11:49.440 --> 00:11:52.140
effectively what you're saying well I

00:11:52.140 --> 00:11:53.339
mean I'm calling for something similar

00:11:53.339 --> 00:11:56.339
which is a global organization uh kind

00:11:56.339 --> 00:11:58.140
of like the IMF or an international

00:11:58.140 --> 00:12:00.240
atomic energy agency

00:12:00.240 --> 00:12:02.940
um where you have a lot of experts you

00:12:02.940 --> 00:12:04.079
have a lot of people in government you

00:12:04.079 --> 00:12:06.420
have a lot of people in uh the companies

00:12:06.420 --> 00:12:07.920
and yeah you have regular meetings

00:12:07.920 --> 00:12:09.540
you're like well this week the new thing

00:12:09.540 --> 00:12:11.279
is this thing called this is a real

00:12:11.279 --> 00:12:13.500
example called Auto gbt where you have

00:12:13.500 --> 00:12:15.839
ai's training other AIS what do we do

00:12:15.839 --> 00:12:17.700
about that how big a threat is it is it

00:12:17.700 --> 00:12:19.680
a small threat big threat like if you

00:12:19.680 --> 00:12:21.180
have a research arm then you can say

00:12:21.180 --> 00:12:23.279
let's do some experiments here and try

00:12:23.279 --> 00:12:25.260
to figure out what the limits are right

00:12:25.260 --> 00:12:27.300
now instead you have like 193 countries

00:12:27.300 --> 00:12:28.920
maybe some of them have read the news

00:12:28.920 --> 00:12:30.959
about this major news Discovery some of

00:12:30.959 --> 00:12:32.459
them haven't even aren't even aware of

00:12:32.459 --> 00:12:34.260
it and there's like no coordination here

00:12:34.260 --> 00:12:35.940
that just can't be the right way you're

00:12:35.940 --> 00:12:37.740
nodding Evan because this is the key

00:12:37.740 --> 00:12:40.320
issue it's miles as miles discussed it's

00:12:40.320 --> 00:12:42.360
not human competitive intelligence it's

00:12:42.360 --> 00:12:45.000
it's what happens after AI gets smarter

00:12:45.000 --> 00:12:46.800
than humor intelligence right amazing

00:12:46.800 --> 00:12:48.839
but you know I I can't go to a

00:12:48.839 --> 00:12:50.279
conference I I actually live in

00:12:50.279 --> 00:12:52.079
Washington DC most the time I can't go

00:12:52.079 --> 00:12:54.360
to a dinner or a conference a meeting

00:12:54.360 --> 00:12:56.279
without the word I being discussed and

00:12:56.279 --> 00:12:57.899
they're all talking about chat GPT and

00:12:57.899 --> 00:13:00.480
Gary's right is that even Auto gppt

00:13:00.480 --> 00:13:02.880
there was a an experiment run last week

00:13:02.880 --> 00:13:04.920
called chaos GPT where they took a

00:13:04.920 --> 00:13:07.200
neutered version of Auto GPT and told it

00:13:07.200 --> 00:13:08.760
to go out and figure out the most

00:13:08.760 --> 00:13:10.260
efficient way to destroy Humanity it was

00:13:10.260 --> 00:13:12.180
a it was sort of a test and it's set to

00:13:12.180 --> 00:13:14.519
work doing it there's there's a lot of

00:13:14.519 --> 00:13:17.820
this stuff is moving incredibly fast and

00:13:17.820 --> 00:13:20.279
figuring out how you can educate policy

00:13:20.279 --> 00:13:23.760
makers about how to mitigate regulate

00:13:23.760 --> 00:13:25.320
bring transparency to some of those

00:13:25.320 --> 00:13:28.680
threats while not preventing what can be

00:13:28.680 --> 00:13:31.920
breathtaking advances in

00:13:31.920 --> 00:13:33.300
um how we live our lives in much more

00:13:33.300 --> 00:13:34.740
fulfilling and purposeful ways and

00:13:34.740 --> 00:13:36.240
Society I think that's that's a lot of

00:13:36.240 --> 00:13:38.160
the trick here to Echo Jack's point

00:13:38.160 --> 00:13:40.079
though about white papers and the way

00:13:40.079 --> 00:13:42.959
you know government moves I I tend to

00:13:42.959 --> 00:13:44.220
agree you know miles may be more

00:13:44.220 --> 00:13:45.480
optimistic than I am but I tend to agree

00:13:45.480 --> 00:13:47.700
I think a lot of the big changes that

00:13:47.700 --> 00:13:48.899
are going to need to happen probably

00:13:48.899 --> 00:13:52.019
won't happen until uh there's some sort

00:13:52.019 --> 00:13:54.360
of provoking event some sort of Crisis I

00:13:54.360 --> 00:13:56.100
don't think that though prevents us from

00:13:56.100 --> 00:13:58.800
starting to have the conversations at

00:13:58.800 --> 00:14:00.300
least the Way Washington tends to work

00:14:00.300 --> 00:14:03.240
at least you want to have the the policy

00:14:03.240 --> 00:14:06.000
container the framework the ideas ready

00:14:06.000 --> 00:14:08.459
some sort of consensus being built so

00:14:08.459 --> 00:14:09.899
that when the opportunity presents

00:14:09.899 --> 00:14:12.540
itself kind of like a a VC who sees a

00:14:12.540 --> 00:14:13.620
great startup right when the opportunity

00:14:13.620 --> 00:14:15.540
presents itself you're ready to jump on

00:14:15.540 --> 00:14:16.800
it you're ready to move forward and I

00:14:16.800 --> 00:14:18.779
think that that has to be happening

00:14:18.779 --> 00:14:20.360
right now

00:14:20.360 --> 00:14:22.200
go ahead

00:14:22.200 --> 00:14:24.420
the opportunity that I see right now is

00:14:24.420 --> 00:14:26.160
to build some Global governance I think

00:14:26.160 --> 00:14:28.139
you have the governments are afraid of

00:14:28.139 --> 00:14:29.880
the technology companies the technology

00:14:29.880 --> 00:14:31.380
companies are afraid the governments are

00:14:31.380 --> 00:14:32.579
going to shut them down as they did in

00:14:32.579 --> 00:14:34.800
Italy and this means everybody has some

00:14:34.800 --> 00:14:37.200
incentive to go to the table that's rare

00:14:37.200 --> 00:14:38.880
and I think we should be seizing that

00:14:38.880 --> 00:14:40.980
opportunity right now to try to do

00:14:40.980 --> 00:14:42.839
something coherent that is dynamic

00:14:42.839 --> 00:14:44.760
enough to cope with the speed of the

00:14:44.760 --> 00:14:46.560
change to take advantage of the good

00:14:46.560 --> 00:14:48.240
things and and to avoid the bad things

00:14:48.240 --> 00:14:50.519
but we need that coordination now and we

00:14:50.519 --> 00:14:52.199
can't just leave this to the usual

00:14:52.199 --> 00:14:54.360
mechanisms it's just too slow miles one

00:14:54.360 --> 00:14:55.740
of the more worrying things that you

00:14:55.740 --> 00:14:58.260
said was that speaking McCarthy was

00:14:58.260 --> 00:15:01.079
looking at an AI for for republicans and

00:15:01.079 --> 00:15:02.760
you know one of the experiences we have

00:15:02.760 --> 00:15:05.279
of recent years is that the Russians

00:15:05.279 --> 00:15:07.079
were able to interfere in a democracy

00:15:07.079 --> 00:15:10.139
and who knows arguably it's been debated

00:15:10.139 --> 00:15:11.699
whether they were able to change some of

00:15:11.699 --> 00:15:12.959
the results through what they were

00:15:12.959 --> 00:15:15.360
putting onto the internet I mean we're

00:15:15.360 --> 00:15:17.579
into a whole new ball game for democracy

00:15:17.579 --> 00:15:20.820
if AI can put out misinformation and

00:15:20.820 --> 00:15:22.019
propaganda

00:15:22.019 --> 00:15:24.839
it's not if it's how much and when it's

00:15:24.839 --> 00:15:27.120
going to happen probably in West 2024

00:15:27.120 --> 00:15:29.760
election wow yeah there's no question I

00:15:29.760 --> 00:15:32.100
mean this this coming election cycle in

00:15:32.100 --> 00:15:34.440
the United States it's a big concern for

00:15:34.440 --> 00:15:36.779
election security authorities it should

00:15:36.779 --> 00:15:38.760
be but I yeah I got to go back to what

00:15:38.760 --> 00:15:40.620
the other panelists said in order to

00:15:40.620 --> 00:15:42.540
respond to it effectively we've got to

00:15:42.540 --> 00:15:44.699
start with education and right now I

00:15:44.699 --> 00:15:46.560
mean I've tried to brief policy makers

00:15:46.560 --> 00:15:48.660
on this it's like explaining particle

00:15:48.660 --> 00:15:50.880
physics to a chocolate chip cookie I

00:15:50.880 --> 00:15:53.160
mean there's just not recognition about

00:15:53.160 --> 00:15:54.899
what's happening if I was President Joe

00:15:54.899 --> 00:15:56.579
Biden right now I will put I would put

00:15:56.579 --> 00:15:58.740
the entire cabinet on Air Force One I

00:15:58.740 --> 00:16:00.480
would fly them to Silicon Valley and we

00:16:00.480 --> 00:16:02.579
would spend the week educate educating

00:16:02.579 --> 00:16:03.839
them about what's happening because

00:16:03.839 --> 00:16:05.459
there aren't just these security

00:16:05.459 --> 00:16:07.860
implications for the elections as Evan

00:16:07.860 --> 00:16:09.540
notes there's also really positive

00:16:09.540 --> 00:16:11.160
implications I mean there's the ability

00:16:11.160 --> 00:16:13.680
to address major health care problems

00:16:13.680 --> 00:16:16.800
hunger homelessness and to do it in real

00:16:16.800 --> 00:16:18.480
time and we are missing some

00:16:18.480 --> 00:16:21.060
opportunities by policy makers not being

00:16:21.060 --> 00:16:23.279
educated on the subject but of course

00:16:23.279 --> 00:16:25.500
security has to come first and in order

00:16:25.500 --> 00:16:27.000
to protect elections or anything else

00:16:27.000 --> 00:16:28.980
it's got to start with you know policy

00:16:28.980 --> 00:16:31.320
makers becoming technologists being

00:16:31.320 --> 00:16:32.399
educated

00:16:32.399 --> 00:16:34.139
fascinating conversation we're gonna

00:16:34.139 --> 00:16:35.820
have to leave it there Evan burfield

00:16:35.820 --> 00:16:37.620
Gary Marcus thank you very much indeed

00:16:37.620 --> 00:16:39.980
for joining us

